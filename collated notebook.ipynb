{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b051e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "# API and KEY file\n",
    "files.upload()                                                                  # Upload the API-KEY File\n",
    "\n",
    "!mkdir -p ~/.kaggle\n",
    "!cp kaggle.json ~/.kaggle/\n",
    "!chmod 600 ~/.kaggle/kaggle.json\n",
    "\n",
    "# Dataset\n",
    "!kaggle datasets download -d mohamedbakhet/amazon-books-reviews                 # Downloading the dataset (zip file)\n",
    "!unzip amazon-books-reviews.zip                                                 # Unzipping the dataset\n",
    "\n",
    "########################################################################\n",
    "\n",
    "! pip install sparknlp\n",
    "\n",
    "import sparknlp\n",
    "spark = sparknlp.start()\n",
    "\n",
    "#########################################################################\n",
    "\n",
    "# ----------------------------\n",
    "# Spark & DataFrame Operations\n",
    "# ----------------------------\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, rand, size, length, expr, lit, split, array, array_except, struct,\n",
    "    collect_list, regexp_replace, lower, monotonically_increasing_id,\n",
    "    sha1, concat_ws, xxhash64, udf\n",
    ")\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "\n",
    "# --------------------\n",
    "# Spark MLlib Features\n",
    "# --------------------\n",
    "from pyspark.ml.feature import Tokenizer, HashingTF, IDF, MinHashLSH\n",
    "\n",
    "# ----------------\n",
    "# NLP - NLTK Setup\n",
    "# ----------------\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer  # for lemmatization\n",
    "from nltk.corpus import stopwords\n",
    "import re                                # regex\n",
    "\n",
    "# NLTK corpora\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# --------\n",
    "# Logging\n",
    "# --------\n",
    "import logging\n",
    "\n",
    "from time import time\n",
    "\n",
    "#########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration Parameters\n",
    "\n",
    "# these are basic default parameters of the main function, however we ran the function for different sample fractions\n",
    "# and passed the values to function arguments\n",
    "# directly when calling the functions.\n",
    "\n",
    "USE_SAMPLE_DATA = True                          # Toggle to use full dataset\n",
    "SAMPLE_FRACTION = 0.01                          # fraction of dataset to use for analysis if working on a sample\n",
    "REVIEW_LENGTH = 50                              # Review length used to filter very short reviews\n",
    "TOKEN_SIZE = 5                                  # Dropping very short lemmatized review (taken to be 5)\n",
    "FEATURES = 4096                                 # Features in HashingTF\n",
    "HASHTABLES = 5                                  # HASHTABLES used for LSH\n",
    "SIM_THRESHOLD = 0.6                             # Similarity threshold for JACCARD Distance\n",
    "path = \"Books_rating.csv\"\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "# Reset logging handlers (important in notebooks or repeated runs)\n",
    "for handler in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(handler)\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(message)s',\n",
    "    handlers=[logging.StreamHandler(sys.stdout)]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80631d51",
   "metadata": {},
   "source": [
    "##### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data Loading using Function\n",
    "def load_sample_preprocess_data(path, USE_SAMPLE_DATA, SAMPLE_FRACTION, seed=42):\n",
    "    # Loading the dataset\n",
    "    df = spark.read.csv(path, header=True)                              # path is path of the file\n",
    "\n",
    "    cleaned_df = df.filter(col(\"review/text\").isNotNull())              # Removing the Nulls from review/text\n",
    "    #cleaned_df.persist()                                                # Caching the cleaned_df\n",
    "\n",
    "    # print(f\"Columns in the dataset are: {cleaned_df.columns}\")          # Column Names in the dataset\n",
    "\n",
    "    if USE_SAMPLE_DATA:\n",
    "        sampled_df = cleaned_df.sample(withReplacement=False, fraction=SAMPLE_FRACTION, seed=seed)          # Sampling Dataset as per given fraction of sample\n",
    "        sampled_df.persist()                                                  # caching the sampled_df\n",
    "    else:\n",
    "        sampled_df = cleaned_df                                                                  # if no sampling then use full dataset\n",
    "\n",
    "    #print(f\"Number of sampled dataset rows are : {sampled_df.count()}\")                         # Count of sampled dataset without nulls\n",
    "\n",
    "    #cleaned_df.unpersist()                                                # UnCaching the cleaned_df (as it is no longer needed)\n",
    "\n",
    "    # preprocessing to create a review_id by comnbining the three columns and hashing ti to reduce memory space\n",
    "\n",
    "    sampled_df = sampled_df.withColumn(\n",
    "        \"review_id\",\n",
    "        xxhash64(sha1(concat_ws(\"_\", col(\"Id\"), col(\"User_id\"), col(\"review/time\"))))                      # concatenating with \"_\" and then hashing (64 bits)\n",
    "    )\n",
    "\n",
    "    return sampled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data sampling\n",
    "def df_selection(sampled_df, REVIEW_LENGTH):\n",
    "\n",
    "    selected_reviews = sampled_df.select(                              # Selecting only book_id and review/text from the sampled dataset\n",
    "    col(\"Id\").alias(\"book_id\"),\n",
    "    col(\"review/text\").alias(\"review_text\"),\n",
    "    col(\"review_id\")\n",
    "    )\n",
    "\n",
    "    # Filtering the reviews with length > 50 characters                             # Rejecting very short reviews from the analysis\n",
    "    filtered_reviews = selected_reviews.filter(\n",
    "        length(col(\"review_text\")) > REVIEW_LENGTH                                  # set initially and passed in function\n",
    "    )\n",
    "\n",
    "    #filtered_reviews = filtered_reviews.dropDuplicates([\"review_text\"])             # Dropping any duplicates from the dataset (if any in just review_text)\n",
    "    filtered_reviews = filtered_reviews.dropDuplicates([\"review_id\"])               # Dropping any duplicates from the dataset (if any in review_id (rare but still))\n",
    "\n",
    "    # filtered_reviews.persist()                                                      # Caching in memory the required dataset\n",
    "\n",
    "    # Show the results\n",
    "    # logging.info(f\"Total Sampled Book Reviews for Similarity Search : {filtered_reviews.count()}\")             # Overview (Count) of the filtered set\n",
    "\n",
    "    return filtered_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d943e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for text processing\n",
    "def text_processing(filtered_reviews, stopwords, TOKEN_SIZE):\n",
    "  stopword_array = array(*[lit(w) for w in stopwords])  # Convert list to Spark array\n",
    "\n",
    "  # clean and lemmatize\n",
    "\n",
    "  lemmatized_reviews_clean = ( filtered_reviews\n",
    "                              .withColumn(\"cleaned_text\", regexp_replace(col(\"review_text\"), r'[^a-zA-Z\\s]', ' '))         # Remove non-alphabetic chars\n",
    "                              .withColumn(\"tokens\", split(lower(col(\"cleaned_text\")), \" \"))                                # Convert text to lowercase and split it into tokens\n",
    "                              .withColumn(\"tokens\", expr(\"FILTER(tokens, x -> LENGTH(x) > 2 AND x != '')\"))                # Filter out tokens that are too short\n",
    "                              .withColumn(\"tokens\", lemmatize_udf(col(\"tokens\")))                                          # Apply lemmatization\n",
    "                              .withColumn(\"tokens\", array_except(col(\"tokens\"), stopword_array))                           # Remove stopwords\n",
    "                              .withColumn(\"token_size\", size(col(\"tokens\")))                                               # Calculate the size of the token array\n",
    "                              .filter((col(\"token_size\") >= TOKEN_SIZE) & (col(\"token_size\") <= 2000))                     # token count range\n",
    "                              .persist()\n",
    "                              )\n",
    "  return lemmatized_reviews_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac4d871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create hash functions\n",
    "\n",
    "import random\n",
    "\n",
    "def create_hash_functions(num_hashes, max_hash=2**32 - 1):\n",
    "    \"\"\"Create a list of hash functions in the form: h(x) = (a*x + b) % p\"\"\"\n",
    "    p = 4294967311  # A large prime > 2^32\n",
    "    hash_funcs = []\n",
    "    random.seed(42)\n",
    "\n",
    "    for _ in range(num_hashes):\n",
    "        a = random.randint(1, p - 1)\n",
    "        b = random.randint(0, p - 1)\n",
    "        hash_funcs.append(lambda x, a=a, b=b: ((a * x + b) % p) % max_hash)\n",
    "    \n",
    "    return hash_funcs\n",
    "\n",
    "hash_funcs = create_hash_functions(num_hashes=100)\n",
    "type(hash_funcs)\n",
    "\n",
    "# Min Hashing\n",
    "\n",
    "def minhash_signature(tokens, hash_funcs):\n",
    "    \"\"\"Generate MinHash signature for a list of tokens.\"\"\"\n",
    "    signature = []\n",
    "\n",
    "    for func in hash_funcs:\n",
    "        min_hash = float('inf')\n",
    "        for token in tokens:\n",
    "            h = func(hash(token))  # Use built-in hash for token-to-int\n",
    "            if h < min_hash:\n",
    "                min_hash = h\n",
    "        signature.append(min_hash)\n",
    "    \n",
    "    return signature\n",
    "\n",
    "# Integrating with pyspark\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "from pyspark.sql.types import ArrayType, LongType\n",
    "import pandas as pd\n",
    "\n",
    "@pandas_udf(ArrayType(LongType()))\n",
    "def compute_minhash_udf(tokens_series: pd.Series) -> pd.Series:\n",
    "    return tokens_series.apply(lambda tokens: minhash_signature(tokens, hash_funcs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca593f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hash tokens \n",
    "def hash_tokens(lemmatized_reviews_clean, FEATURES):\n",
    "    # FEATURES now means \"number of hash functions\" (e.g., 100)\n",
    "    hashed_reviews = lemmatized_reviews_clean.withColumn(\n",
    "        \"signature\", compute_minhash_udf(col(\"tokens\"))\n",
    "    )\n",
    "    return hashed_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a39c585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSH\n",
    "from collections import defaultdict\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import hashlib\n",
    "\n",
    "class LSH:\n",
    "    def __init__(self, num_bands):\n",
    "        self.b = num_bands\n",
    "        self.buckets = [defaultdict(list) for _ in range(num_bands)]\n",
    "        self.r = None\n",
    "        self.id_counter = 0\n",
    "\n",
    "    def _hash_band(self, band):\n",
    "        return hashlib.md5(str(band).encode()).hexdigest()\n",
    "\n",
    "    def add_signature(self, signature):\n",
    "        if self.r is None:\n",
    "            assert len(signature) % self.b == 0, \"Signature length must divide evenly into bands\"\n",
    "            self.r = len(signature) // self.b\n",
    "\n",
    "        bands = [signature[i*self.r:(i+1)*self.r] for i in range(self.b)]\n",
    "\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            hashes = list(executor.map(self._hash_band, bands))\n",
    "\n",
    "        for i, h in enumerate(hashes):\n",
    "            self.buckets[i][h].append(self.id_counter)\n",
    "\n",
    "        self.id_counter += 1\n",
    "\n",
    "    def get_candidate_pairs(self):\n",
    "        pairs = set()\n",
    "        for bucket in self.buckets:\n",
    "            for ids in bucket.values():\n",
    "                if len(ids) > 1:\n",
    "                    for i in range(len(ids)):\n",
    "                        for j in range(i + 1, len(ids)):\n",
    "                            pairs.add(tuple(sorted((ids[i], ids[j]))))\n",
    "        return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcf344d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ee5311",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008dd764",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
